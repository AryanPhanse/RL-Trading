{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09305a9c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# 1. Load Stock Data\n",
    "def load_data(ticker='TCS.NS', start='2018-01-01', end='2020-01-01'):\n",
    "    df = yf.download(ticker, start=start, end=end)\n",
    "    # Add technical indicators that can be used as features\n",
    "    df['Returns'] = df['Close'].pct_change()\n",
    "    # Calculate moving averages\n",
    "    df['MA5'] = df['Close'].rolling(window=5).mean()\n",
    "    df['MA10'] = df['Close'].rolling(window=10).mean()\n",
    "    # Add more features as needed\n",
    "    df.dropna(inplace=True)\n",
    "    return df\n",
    "\n",
    "# 2. Define TDQN Network\n",
    "class DQNNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQNNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 512)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc3 = nn.Linear(512, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# 3. TDQN Agent\n",
    "class TDQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.gamma = 0.6  # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.05\n",
    "        self.epsilon_decay = 0.999\n",
    "        self.learning_rate = 0.0001\n",
    "        self.model = DQNNetwork(state_size, action_size)\n",
    "        self.target_model = DQNNetwork(state_size, action_size)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.update_target_network()\n",
    "        \n",
    "    def update_target_network(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def act(self, state, training=True):\n",
    "        if training and np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        act_values = self.model(state)\n",
    "        return torch.argmax(act_values).item()\n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        \n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*minibatch)\n",
    "        \n",
    "        states = torch.FloatTensor(states)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        actions = torch.LongTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        dones = torch.FloatTensor(dones)\n",
    "        \n",
    "        # Current Q values\n",
    "        current_q = self.model(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # Next Q values using target network\n",
    "        next_q = self.target_model(next_states).max(1)[0]\n",
    "        target_q = rewards + (1 - dones) * self.gamma * next_q\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = nn.MSELoss()(current_q, target_q.detach())\n",
    "        \n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Update epsilon\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "# 4. Trading Environment\n",
    "class TradingEnvironment:\n",
    "    def __init__(self, df, initial_capital=100000):\n",
    "        self.df = df\n",
    "        self.initial_capital = initial_capital\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.position = 0  # 0: no position, 1: long, -1: short\n",
    "        self.capital = float(self.initial_capital)\n",
    "        self.current_step = 0\n",
    "        self.portfolio_values = [float(self.initial_capital)]\n",
    "        self.buy_signals = []\n",
    "        self.sell_signals = []\n",
    "        return self._get_state()\n",
    "    \n",
    "    def _get_state(self):\n",
    "        # Create state from current data window\n",
    "        features = [\n",
    "            float(self.df['Returns'].iloc[self.current_step]),\n",
    "            float(self.df['MA5'].iloc[self.current_step] / self.df['Close'].iloc[self.current_step] - 1),\n",
    "            float(self.df['MA10'].iloc[self.current_step] / self.df['Close'].iloc[self.current_step] - 1),\n",
    "            float(self.position)\n",
    "        ]\n",
    "        return np.array(features, dtype=np.float32)\n",
    "    \n",
    "    def step(self, action):\n",
    "        # 0: short, 1: long\n",
    "        current_price = self.df['Close'].iloc[self.current_step]\n",
    "    \n",
    "        # Calculate reward based on position and price change\n",
    "        next_step = min(self.current_step + 1, len(self.df) - 1)\n",
    "        next_price = self.df['Close'].iloc[next_step]\n",
    "        price_change = float((next_price / current_price) - 1)\n",
    "    \n",
    "        # Update position based on action\n",
    "        old_position = self.position\n",
    "        if action == 0:  # Short\n",
    "            if old_position != -1:  # Only record if position changes\n",
    "                self.position = -1\n",
    "                self.sell_signals.append(self.current_step)\n",
    "        else:  # Long\n",
    "            if old_position != 1:  # Only record if position changes\n",
    "                self.position = 1\n",
    "                self.buy_signals.append(self.current_step)\n",
    "    \n",
    "        # Calculate reward with higher incentive for correct trades\n",
    "        if (self.position == 1 and price_change > 0) or (self.position == -1 and price_change < 0):\n",
    "            reward = abs(price_change) * 2  # Bonus for correct direction\n",
    "        else:\n",
    "            reward = self.position * price_change\n",
    "    \n",
    "        # Update capital with more realistic returns\n",
    "        self.capital *= (1 + self.position * price_change * 0.95)  # 0.95 to account for slippage/fees\n",
    "        self.portfolio_values.append(float(self.capital))\n",
    "    \n",
    "        # Move to next step\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= len(self.df) - 1\n",
    "    \n",
    "        return self._get_state(), reward, done, {}\n",
    "\n",
    "# 5. Train TDQN Agent\n",
    "def train_tdqn_agent(df, episodes=100):\n",
    "    # Define state and action sizes\n",
    "    state_size = 4  # Adjust based on your features\n",
    "    action_size = 2  # Long or Short\n",
    "    \n",
    "    # Create agent and environment\n",
    "    agent = TDQNAgent(state_size, action_size)\n",
    "    env = TradingEnvironment(df)\n",
    "    \n",
    "    batch_size = 32\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            \n",
    "            # Train the agent\n",
    "            if len(agent.memory) > batch_size:\n",
    "                agent.replay(batch_size)\n",
    "        \n",
    "        # Update target network periodically\n",
    "        if episode % 2 == 0:\n",
    "            agent.update_target_network()\n",
    "            \n",
    "        # Print progress\n",
    "        if episode % 2 == 0:\n",
    "            print(f\"Episode: {episode}, Final Portfolio Value: {float(env.capital):.2f}\")\n",
    "    \n",
    "    return agent, env\n",
    "\n",
    "# 6. Generate Signals with Trained Agent\n",
    "def generate_signals_with_tdqn(df, agent):\n",
    "    env = TradingEnvironment(df)\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.act(state, training=False)\n",
    "        next_state, _, done, _ = env.step(action)\n",
    "        state = next_state\n",
    "    \n",
    "    # Ensure portfolio_values has the same length as df\n",
    "    portfolio_values = env.portfolio_values\n",
    "    if len(portfolio_values) < len(df):\n",
    "        portfolio_values.append(portfolio_values[-1])  # Duplicate last value if needed\n",
    "    \n",
    "    return env.buy_signals, env.sell_signals, np.array(portfolio_values, dtype=float)\n",
    "\n",
    "# 7. Plot Function (unchanged)\n",
    "def plot_dual_panel(df, buy_signals, sell_signals, portfolio):\n",
    "    dates = df.index\n",
    "    prices = df['Close'].values\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 8), sharex=True)\n",
    "\n",
    "    # Top: Price\n",
    "    ax1.plot(dates, prices, label='Price', color='blue')\n",
    "    ax1.scatter(dates[buy_signals], prices[buy_signals], color='green', marker='^', label='Long')\n",
    "    ax1.scatter(dates[sell_signals], prices[sell_signals], color='red', marker='v', label='Short')\n",
    "    ax1.set_ylabel('Price $P_t$')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # Bottom: Portfolio Value\n",
    "    ax2.plot(dates, portfolio, label='Capital', color='blue')  # Remove the [:-1] slicing\n",
    "    ax2.scatter(dates[buy_signals], portfolio[buy_signals], color='green', marker='^', label='Long')\n",
    "    ax2.scatter(dates[sell_signals], portfolio[sell_signals], color='red', marker='v', label='Short')\n",
    "    ax2.set_ylabel('Capital $V_t$')\n",
    "    ax2.set_xlabel('Timestamp')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run\n",
    "ticker = 'TCS.NS'\n",
    "df = load_data(ticker, start='2018-01-01', end='2020-01-01')\n",
    "\n",
    "# Train the TDQN agent\n",
    "agent, _ = train_tdqn_agent(df, episodes=100)\n",
    "\n",
    "# Generate signals with trained agent\n",
    "buy_signals, sell_signals, portfolio = generate_signals_with_tdqn(df, agent)\n",
    "\n",
    "# Plot results\n",
    "plot_dual_panel(df, buy_signals, sell_signals, portfolio)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
